---
title: "CrossValidation"
format: html
---

```{r}
library(tidyverse)
library(tidymodels)
library(readxl)
breastcancer = read_csv("breastcancer.csv")
```

Write a brief introduction to cross validation which includes relevant mathematical notation.

Cross-validation is a class of methods that estimate the test error by rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.

$$\text{CV}_{(k)} = \frac{1}{k} \sum_{j=1}^k \text{MAE}_j$$

What is the goal of cross-validation?

The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give insight into how the model will generalize to an independent data set. Overﬁtting occurs when a model reads too much into chance features and essentially memorizes features of the data used to build it, thus reducing reliability and the ability to generalize to underrepresented populations. Cross-validation helps prevent this by providing a more reliable estimate of how well a model would generalize to unseen data.

What linear models are you considering based on your research question? Pick at least two models to compare:

E\[perimeter_height \| concavity_mean\] = β0+ β1(concavity_mean)

```{r}
lm_spec <- linear_reg() %>% set_mode('regression') %>% set_engine('lm')
```

```{r}
cancer_model<-lm_spec %>% 
  fit(perimeter_mean ~ concavity_mean, data = breastcancer)
```

```{r}
cancer_model <- ggplot(breastcancer, aes(x = concavity_mean, y = perimeter_mean)) + geom_point()

cancer_model
```

```{r}
# Model _1: 1 predictor (Y = b0 + b1 X)
cancer_model + geom_smooth(method = "lm", se = FALSE)
```

```{r}
# Model_2: 2 predictors (Y = b0 + b1 X + b2 X^2)
cancer_model + geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 2))
```

```{r}
# Model_3: 10 predictors (Y = b0 + b1 X + b2 X^2 + ... + b10 X^10)
cancer_model + geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 10))
```
The models above demonstrate that the more predictors are added, the more the models become overfit to the noise in our data. The models then lose their ability to generalize to new data.

Explain how you divided your data into its test set and training set.

```{r}
breastcancer %>% 
  select(perimeter_mean, texture_mean, smoothness_mean, concavity_mean, diagnosis)
```

We have created an 80-20 train-test split below. This ensures that there is a substantial amount of training data in order to evaluate a decently sized testing set.

```{r}
# Set the random number seed
set.seed(8)

# Split the data into 80% / 20%
# Ensure that the sub-samples are similar with respect to mpg
cancer_split = initial_split(breastcancer, strata = perimeter_mean, prop = .8)
```

```{r}
# Get the training data from the split
cancer_train = training(cancer_split)

# Get the testing data from the split
cancer_test = testing(cancer_split)
```

```{r}
nrow(breastcancer)
nrow(cancer_train)
nrow(cancer_test)
```

```{r}
lm_spec <- linear_reg() %>% set_mode('regression') %>% set_engine('lm')
```

```{r}
# Model estimation
cancer_model <- lm_spec %>% 
  fit(perimeter_mean ~ concavity_mean, data = breastcancer)
```

```{r}
model_10_train <- lm_spec %>% 
  fit(perimeter_mean ~ poly(concavity_mean, 10), data = breastcancer)
```

```{r}
# How well does the TRAINING model predict the TRAINING data?
# Calculate the training (in-sample) MAE
model_10_train %>% 
  augment(new_data = cancer_train) %>% 
  mae(truth = perimeter_mean, estimate = .pred)

# How well does the TRAINING model predict the TEST data?
# Calculate the test MAE
model_10_train %>% 
  augment(new_data = cancer_test) %>% 
  mae(truth = perimeter_mean, estimate = .pred)
```

State which error metric you are using (MAE or MSE) and give its formal mathematical definition. Why did you choose this error metric? What are the advantages/disadvantages of using it?

We are using MAE, because it provides a fair and interpretable measure of error without disproportionately punishing large outliers, which makes it suitable for medical datasets such as this one, for breast cancer tumors. 
MAE = (1/n) Σ(i=1 to n) |y_i – ŷ_i|

Implement k-fold cross validation for k = 10.

```{r}
set.seed(244)

cancer_model_cv = lm_spec %>%
fit_resamples(
  perimeter_mean ~ concavity_mean, 
  resamples = vfold_cv(breastcancer, v = 10), 
  metrics = metric_set(mae, rmse, rsq)
  )
```

```{r}
cancer_model_cv %>% collect_metrics()
```

```{r}
cancer_model_cv %>% unnest(.metrics) %>% 
filter(.metric == "mae")
```

Based on my random folds above, the prediction error (MAE) was best for fold 10 and worst for fold 5. On average, the MAE is 12.758

```{r}
# 10-fold cross-validation for model_1
set.seed(244)
model_1_cv <- lm_spec %>% 
  fit_resamples(
    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,
    resamples = vfold_cv(breastcancer, v = 10), 
    metrics = metric_set(mae, rsq)
  )
```

```{r}
# 10-fold cross-validation for model_2
set.seed(253)
model_2_cv <- lm_spec %>% 
  fit_resamples(
    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,
    resamples = vfold_cv(breastcancer, v = 10), 
    metrics = metric_set(mae, rsq)
  )
```

```{r}
model_1_cv %>% 
  collect_metrics()
model_2_cv %>% 
  collect_metrics()
```

Display evaluation metrics for your different models in a clean, organized way. This display should include both the estimated CV metric as well as its standard deviation.

```{r}
# STEP 2: model estimation
model_1 <- lm_spec %>% 
  fit(perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean, data = breastcancer)

model_2 <- lm_spec %>% 
  fit(perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean, data = breastcancer)

model_3 <- lm_spec %>% 
  fit(perimeter_mean ~ texture_mean + concavity_mean, data=breastcancer)
```

```{r}
# IN-SAMPLE R^2 for model_1 = ???
model_1 %>% glance()

# IN-SAMPLE R^2 for model_2 = ???
model_2 %>% glance()

model_3 %>% glance()
```

```{r}
# IN-SAMPLE MAE for model_1 = 
model_1 %>% 
  augment(new_data = breastcancer) %>% 
  mae(truth = perimeter_mean, estimate = .pred)

# IN-SAMPLE MAE for model_2 = 
model_2 %>% 
  augment(new_data = breastcancer) %>% 
  mae(truth = perimeter_mean, estimate = .pred)
```

Try different values of k (the tuning parameter). At minimum, try k = n - 1 (LOOCV), and k = 5. Which value of k has the smallest CV error?

```{r}
model_1_loocv <- lm_spec %>% 
  fit_resamples(
    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,
    resamples = vfold_cv(breastcancer, v = 40), 
    metrics = metric_set(mae)
  )
```

```{r}
model_1_loocv %>% collect_metrics
```

```{r}
# 5-fold cross-validation for model_1
set.seed(244)

cancer_model_cv = lm_spec %>%
fit_resamples(
  perimeter_mean ~ concavity_mean, 
  resamples = vfold_cv(breastcancer, v = 5), 
  metrics = metric_set(mae, rmse, rsq)
  )
```

```{r}
cancer_model_cv %>% collect_metrics()
```

```{r}
cancer_model_cv %>% unnest(.metrics) %>% 
filter(.metric == "mae")
```

Based on my random folds above, the prediction error (MAE) was best for fold 4 and worst for fold 3.

```{r}
# 5-fold cross-validation for model_1
set.seed(244)
model_1_cv <- lm_spec %>% 
  fit_resamples(
    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,
    resamples = vfold_cv(breastcancer, v = 5), 
    metrics = metric_set(mae, rsq)
  )
```

```{r}
# 5-fold cross-validation for model_2
set.seed(253)
model_2_cv <- lm_spec %>% 
  fit_resamples(
    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,
    resamples = vfold_cv(breastcancer, v = 5), 
    metrics = metric_set(mae, rsq)
  )
```

```{r}
# 5-fold cross-validation for model_3
set.seed(253)
model_3_cv <- lm_spec %>% 
  fit_resamples(
    perimeter_mean ~ texture_mean + concavity_mean,
    resamples = vfold_cv(breastcancer, v = 5), 
    metrics = metric_set(mae, rsq)
  )
```


```{r}
model_1_cv %>% 
  collect_metrics()
model_2_cv %>% 
  collect_metrics()
model_3_cv %>% 
  collect_metrics()
```

```{r}
# 8-fold cross-validation
set.seed(244)

cancer_model_cv = lm_spec %>%
fit_resamples(
  perimeter_mean ~ concavity_mean, 
  resamples = vfold_cv(breastcancer, v = 8), 
  metrics = metric_set(mae, rmse, rsq)
  )
```

```{r}
cancer_model_cv %>% collect_metrics()
```

```{r}
cancer_model_cv %>% unnest(.metrics) %>% 
filter(.metric == "mae")
```

Based on my random folds above, the prediction error (MAE) was best for fold 2 and worst for fold 1.

```{r}
# 8-fold cross-validation for model_1
set.seed(244)
model_1_cv <- lm_spec %>% 
  fit_resamples(
    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,
    resamples = vfold_cv(breastcancer, v = 8), 
    metrics = metric_set(mae, rsq)
  )
```

```{r}
# 8-fold cross-validation for model_2
set.seed(244)
model_2_cv <- lm_spec %>% 
  fit_resamples(
    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,
    resamples = vfold_cv(breastcancer, v = 8), 
    metrics = metric_set(mae, rsq)
  )
```

```{r}
model_1_cv %>% 
  collect_metrics()
model_2_cv %>% 
  collect_metrics()
```

The CV errors seem to be fairly consistent between 10-fold, 5-fold, 8-fold and LOOCV. Although a slim difference, the 10-fold cross-validation measure provides the smallest CV error. 

Select your final model based on which one has the smallest CV error.

For model_1, it looks like the MAE is roughly similar for when it’s measured in-sample (11.82) versus when it’s tested on “new” data (each test fold held out) (11.887 for 10-fold, 11.92 for 5-fold and LOOCV, and 11.875 for 8-fold CV). Model_2 also has roughly similar MAE for in-sample (11.801) versus CV data (11.93 for 10-fold, 12.01 for 5-fold, and 11.901 for 8-fold CV). Althought the difference is small, it seems better to pick the first model.
