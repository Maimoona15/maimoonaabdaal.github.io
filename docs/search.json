[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT-244",
    "section": "",
    "text": "This is a Quarto website\nHi I’m Maimoona yay\n\n1 + 1 \n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Maimoona and this is my website! I am a sophomore majoring in Psychology & Statistics at Mount Holyoke College :)\nHere is a link to my LinkedIn, where you can get in touch with me! https://www.linkedin.com/in/maimoona-abdaal-693269318/\n\n\n\n\n\n\n\n\nFigure 1: This is me at the Spring Flower Show!"
  },
  {
    "objectID": "maimoonaabdaal.github.io.html",
    "href": "maimoonaabdaal.github.io.html",
    "title": "Welcome to my homepage!",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "maimoonaabdaal.github.io.html#quarto",
    "href": "maimoonaabdaal.github.io.html#quarto",
    "title": "Welcome to my homepage!",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "CP3.html",
    "href": "CP3.html",
    "title": "CP 3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.6     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.4     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(readxl)\nbreastcancer = read_csv(\"breastcancer.csv\")\n\nRows: 569 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): diagnosis\ndbl (31): id, radius_mean, texture_mean, perimeter_mean, area_mean, smoothne...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWrite a brief introduction to cross validation which includes relevant mathematical notation.\nCross-validation is a class of methods that estimate the test error by rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.\n\\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]\nWhat is the goal of cross-validation? (Hint: Think about over-fitting, along with social/ethical considerations)\nThe goal of cross-validation is to test the model’s ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give insight into how the model will generalize to an independent data set. Overﬁtting occurs when a model reads too much into chance features and essentially memorizes features of the data used to build it, thus reducing reliability and the ability to generalize to underrepresented populations.\nWhat linear models are you considering based on your research question? Pick at least two models to compare.\nFor example, in Lab 7, we first considered the following model for black cherry trees:\nE[height | diameter] = β0+ β1(diameter)\nExplain how you divided your data into its test set and training set.\n\nbreastcancer %&gt;% \n  select(perimeter_mean, texture_mean, smoothness_mean, concavity_mean, diagnosis)\n\n# A tibble: 569 × 5\n   perimeter_mean texture_mean smoothness_mean concavity_mean diagnosis\n            &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    \n 1          123.          10.4          0.118          0.300  M        \n 2          133.          17.8          0.0847         0.0869 M        \n 3          130           21.2          0.110          0.197  M        \n 4           77.6         20.4          0.142          0.241  M        \n 5          135.          14.3          0.100          0.198  M        \n 6           82.6         15.7          0.128          0.158  M        \n 7          120.          20.0          0.0946         0.113  M        \n 8           90.2         20.8          0.119          0.0937 M        \n 9           87.5         21.8          0.127          0.186  M        \n10           84.0         24.0          0.119          0.227  M        \n# ℹ 559 more rows\n\n\n\ncancer_plot &lt;- ggplot(breastcancer, aes(x = concavity_mean, y = perimeter_mean)) + geom_point()\n\ncancer_plot\n\n\n\n\n\n\n\n\n\ncancer_plot + geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Model_2: 2 predictors (Y = b0 + b1 X + b2 X^2)\ncancer_plot + geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2))\n\n\n\n\n\n\n\n\n\n# Model_3: 10 predictors (Y = b0 + b1 X + b2 X^2 + ... + b10 X^10)\ncancer_plot + geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 10))\n\n\n\n\n\n\n\n\n\n# Set the random number seed\nset.seed(8)\n\n# Split the cars data into 80% / 20%\n# Ensure that the sub-samples are similar with respect to mpg\ncancer_split = initial_split(breastcancer, strata = perimeter_mean, prop = .8)\n\n\n# Get the training data from the split\ncancer_train = training(cancer_split)\n\n# Get the testing data from the split\ncancer_test = testing(cancer_split)\n\n\nnrow(breastcancer)\n\n[1] 569\n\nnrow(cancer_train)\n\n[1] 453\n\nnrow(cancer_test)\n\n[1] 116\n\n\n\nlm_spec &lt;- linear_reg() %&gt;% set_mode('regression') %&gt;% set_engine('lm')\n\n\n# Step 2: Model estimation\ncancer_model &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ concavity_mean, data = breastcancer)\n\n\nmodel_10_train &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ poly(concavity_mean, 10), data = breastcancer)\n\n\n# How well does the TRAINING model predict the TRAINING data?\n# Calculate the training (in-sample) MAE\nmodel_10_train %&gt;% \n  augment(new_data = cancer_train) %&gt;% \n  mae(truth = perimeter_mean, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        12.3\n\n# How well does the TRAINING model predict the TEST data?\n# Calculate the test MAE\n# YOUR CODE HERE\nmodel_10_train %&gt;% \n  augment(new_data = cancer_test) %&gt;% \n  mae(truth = perimeter_mean, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        12.7\n\n\nState which error metric you are using (MAE or MSE) and give its formal mathematical definition. Why did you choose this error metric? What are the advantages/disadvantages of using it?\nHint: MAEj (the mean absolute error for the jth fold, which has nj observations in it) is the 1 norm of the error vector e = y -y = (y1,…,ynj) - (y1,…, ynj) divided by the number of observations (nj). See your notes from Vector Norms (Blackboard Lecture). Hint: MSEj (mean square error for jth fold, which has nj observations) is the squared 2-norm of the error vector e = y -y = (y1,…,ynj) - (y1,…, ynj) divided by the number of observations (nj). See your notes from Vector Norms (Blackboard Lecture).\nImplement k-fold cross validation for k = 10.\n\nset.seed(244)\n\ncancer_model_cv = lm_spec %&gt;%\nfit_resamples(\n  perimeter_mean ~ concavity_mean, \n  resamples = vfold_cv(breastcancer, v = 10), \n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ncancer_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.8      10  0.519  Preprocessor1_Model1\n2 rmse    standard   16.9      10  0.688  Preprocessor1_Model1\n3 rsq     standard    0.530    10  0.0492 Preprocessor1_Model1\n\n\n\ncancer_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\n# A tibble: 10 × 7\n   splits           id     .metric .estimator .estimate .config         .notes  \n   &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;           &lt;list&gt;  \n 1 &lt;split [512/57]&gt; Fold01 mae     standard        14.5 Preprocessor1_… &lt;tibble&gt;\n 2 &lt;split [512/57]&gt; Fold02 mae     standard        14.2 Preprocessor1_… &lt;tibble&gt;\n 3 &lt;split [512/57]&gt; Fold03 mae     standard        13.0 Preprocessor1_… &lt;tibble&gt;\n 4 &lt;split [512/57]&gt; Fold04 mae     standard        11.4 Preprocessor1_… &lt;tibble&gt;\n 5 &lt;split [512/57]&gt; Fold05 mae     standard        14.8 Preprocessor1_… &lt;tibble&gt;\n 6 &lt;split [512/57]&gt; Fold06 mae     standard        10.7 Preprocessor1_… &lt;tibble&gt;\n 7 &lt;split [512/57]&gt; Fold07 mae     standard        11.8 Preprocessor1_… &lt;tibble&gt;\n 8 &lt;split [512/57]&gt; Fold08 mae     standard        13.8 Preprocessor1_… &lt;tibble&gt;\n 9 &lt;split [512/57]&gt; Fold09 mae     standard        13.6 Preprocessor1_… &lt;tibble&gt;\n10 &lt;split [513/56]&gt; Fold10 mae     standard        10.3 Preprocessor1_… &lt;tibble&gt;\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 10 and worst for fold 5.\n\n# 10-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.0      10  0.544  Preprocessor1_Model1\n2 rsq     standard    0.568    10  0.0499 Preprocessor1_Model1\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   11.9      10  0.528  Preprocessor1_Model1\n2 rsq     standard    0.560    10  0.0327 Preprocessor1_Model1\n\n\nDisplay evaluation metrics for your different models in a clean, organized way. This display should include both the estimated CV metric as well as its standard deviation.\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean, data = breastcancer)\n\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean, data = breastcancer)\n\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.557         0.555  16.2      237. 1.96e-99     3 -2391. 4791. 4813.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.565         0.562  16.1      183. 2.36e-100     4 -2386. 4783. 4809.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n# IN-SAMPLE MAE for model_1 = ???\nmodel_1 %&gt;% \n  augment(new_data = breastcancer) %&gt;% \n  mae(truth = perimeter_mean, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        11.9\n\n# IN-SAMPLE MAE for model_2 = ???\nmodel_2 %&gt;% \n  augment(new_data = breastcancer) %&gt;% \n  mae(truth = perimeter_mean, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        11.9\n\n\nTry different values of k (the tuning parameter). At minimum, try k = n - 1 (LOOCV), and k = 5. Which value of k has the smallest CV error?\n\nmodel_1_loocv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 40), \n    metrics = metric_set(mae)\n  )\n\n\nmodel_1_loocv %&gt;% collect_metrics\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    12.0    40   0.503 Preprocessor1_Model1\n\n\n\n# 5-fold cross-validation for model_1\nset.seed(244)\n\ncancer_model_cv = lm_spec %&gt;%\nfit_resamples(\n  perimeter_mean ~ concavity_mean, \n  resamples = vfold_cv(breastcancer, v = 5), \n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ncancer_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.8       5  0.190  Preprocessor1_Model1\n2 rmse    standard   17.1       5  0.269  Preprocessor1_Model1\n3 rsq     standard    0.527     5  0.0334 Preprocessor1_Model1\n\n\n\ncancer_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\n# A tibble: 5 × 7\n  splits            id    .metric .estimator .estimate .config          .notes  \n  &lt;list&gt;            &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;list&gt;  \n1 &lt;split [455/114]&gt; Fold1 mae     standard        12.7 Preprocessor1_M… &lt;tibble&gt;\n2 &lt;split [455/114]&gt; Fold2 mae     standard        12.9 Preprocessor1_M… &lt;tibble&gt;\n3 &lt;split [455/114]&gt; Fold3 mae     standard        13.5 Preprocessor1_M… &lt;tibble&gt;\n4 &lt;split [455/114]&gt; Fold4 mae     standard        12.5 Preprocessor1_M… &lt;tibble&gt;\n5 &lt;split [456/113]&gt; Fold5 mae     standard        12.6 Preprocessor1_M… &lt;tibble&gt;\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 4 and worst for fold 3.\n\n# 5-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 5), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n# 5-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 5), \n    metrics = metric_set(mae, rsq)\n  )\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.0       5  0.236  Preprocessor1_Model1\n2 rsq     standard    0.569     5  0.0396 Preprocessor1_Model1\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   11.9       5  0.599  Preprocessor1_Model1\n2 rsq     standard    0.556     5  0.0418 Preprocessor1_Model1\n\n\n\n# 8-fold cross-validation\nset.seed(244)\n\ncancer_model_cv = lm_spec %&gt;%\nfit_resamples(\n  perimeter_mean ~ concavity_mean, \n  resamples = vfold_cv(breastcancer, v = 8), \n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ncancer_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.8       8  0.399  Preprocessor1_Model1\n2 rmse    standard   16.9       8  0.765  Preprocessor1_Model1\n3 rsq     standard    0.517     8  0.0406 Preprocessor1_Model1\n\n\n\ncancer_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\n# A tibble: 8 × 7\n  splits           id    .metric .estimator .estimate .config           .notes  \n  &lt;list&gt;           &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n1 &lt;split [497/72]&gt; Fold1 mae     standard        14.8 Preprocessor1_Mo… &lt;tibble&gt;\n2 &lt;split [498/71]&gt; Fold2 mae     standard        11.2 Preprocessor1_Mo… &lt;tibble&gt;\n3 &lt;split [498/71]&gt; Fold3 mae     standard        13.0 Preprocessor1_Mo… &lt;tibble&gt;\n4 &lt;split [498/71]&gt; Fold4 mae     standard        12.2 Preprocessor1_Mo… &lt;tibble&gt;\n5 &lt;split [498/71]&gt; Fold5 mae     standard        13.5 Preprocessor1_Mo… &lt;tibble&gt;\n6 &lt;split [498/71]&gt; Fold6 mae     standard        11.8 Preprocessor1_Mo… &lt;tibble&gt;\n7 &lt;split [498/71]&gt; Fold7 mae     standard        12.5 Preprocessor1_Mo… &lt;tibble&gt;\n8 &lt;split [498/71]&gt; Fold8 mae     standard        13.2 Preprocessor1_Mo… &lt;tibble&gt;\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 2 and worst for fold 1.\n\n# 8-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 8), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n# 8-fold cross-validation for model_2\nset.seed(244)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 8), \n    metrics = metric_set(mae, rsq)\n  )\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   11.9       8  0.461  Preprocessor1_Model1\n2 rsq     standard    0.557     8  0.0418 Preprocessor1_Model1\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.0       8  0.457  Preprocessor1_Model1\n2 rsq     standard    0.553     8  0.0403 Preprocessor1_Model1\n\n\nSelect your final model based on which one has the smallest CV error."
  },
  {
    "objectID": "K-Means-Mini-Demo.html",
    "href": "K-Means-Mini-Demo.html",
    "title": "Lab: K-Means",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue.\n\n\n\n\nLoad Packages\nYou likely will need to install some these packages before you can run the code chunk below successfully.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(amap)\n\n\n\nLoad Penguin Data\n\ndata(penguins)\n\n\n\nData Cleaning\n\n# Remove missing values\n# YOUR CODE HERE\npenguins &lt;- penguins %&gt;%\n            filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm) & !is.na(species))\n\n# Make data table (named penguins_reduced) that only has\n# bill_length_mm and bill_depth_mm columns\npenguins_reduced &lt;- penguins %&gt;% select(bill_length_mm, bill_depth_mm)\n\n\n\nInitial Visualization\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  geom_point() \n\n\n\n\n\n\n\n\nWe’ll cluster these penguins based on their bill lengths and depths:\n\n\nImplement \\(K\\)-Means\nComplete the code below to run the K-means algorithm using K = 3.\n\nset.seed(244)\n# Run the K-means algorithm\nkmeans_3_round_1 &lt;- kmeans(scale(penguins_reduced), centers = 3) \n    \n# Plot the cluster assignments\npenguins_reduced %&gt;% \n  mutate(kmeans_cluster = as.factor(kmeans_3_round_1$cluster)) %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = kmeans_cluster)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"K-means with K = 3 (round 1)\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhy do we have to set the seed for K-means? In practice, why should we try out a variety of seeds?\n\nAnswer. For reproducibility. Every time we run the function kmeans, it initializes the centers of the clusters to be in random locations. It’s possible that some random locations give better clustering results than others; since k-means is a greedy algorithm, it’s possible for it to have a different result each time, and it’s possible for it to get “stuck” at a local solution.\n\n\nK-Means Clusters Versus Known Species Groupings\n\n  ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Actual Groupings of Data Based on Species\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nVisually, how well do you think \\(K\\)-means captured the underlying species structure of the data?\n\nAnswer. I think it did pretty good! It found roughly the data points corresponding to each of the species groups, so it is identifying that as a way to structure the data.\n\n\nTuning \\(K\\)\n\nTo implement K-means clustering we must choose an appropriate K! Use the following example to see the two different extreme situations. Typically, the ideal \\(K\\) is somewhere between the two extremes.\nMinimum: \\(K = 2\\) groups/clusters\nMaximum: \\(K = n\\) groups/clusters (one observation per cluster)\n\nWhat happens in the \\(K\\)-means algorithm if \\(K = n\\)?\nAnswer. YOUR ANSWER HERE\nLet’s consider anywhere from \\(K = 2\\) to \\(K = 20\\) clusters.\n\nset.seed(244)\n\nk_2  &lt;- kmeans(scale(penguins_reduced), centers = 2)\nk_20 &lt;- kmeans(scale(penguins_reduced), centers = 20)\n\npenguins_reduced %&gt;% \n  mutate(cluster_2 = as.factor(k_2$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_2)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 2\")\n\n\n\n\n\n\n\npenguins_reduced %&gt;% \n  mutate(cluster_20 = as.factor(k_20$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_20)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 20\") + \n    scale_color_manual(values = rainbow(20))\n\n\n\n\n\n\n\n\nWhat are your general impressions?\nAnswer. YOUR ANSWER HERE\n\n\nFinding Ideal K Value: Silhoutte\n\nThe average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster.\n\nTo do so, it maximizes the distance between clusters and minimizes distance within clusters.\n\nA high average silhouette indicates a good clustering.\nGiven a range of possible K values, the optimal number of clusters (K) is the one that maximizes the average silhouette.\n\nWe can use a built-in silhouette method in the fviz_nbclust function to compute the average silhouette for various K values.\n\nfviz_nbclust(scale(penguins_reduced), kmeans, method='silhouette')\n\n\n\n\n\n\n\n\nBased on the average silhouette approach, what is the optimal \\(K\\) value?\nAnswer. The optimal value for \\(K\\) appears to be \\(K\\) = 3\n\n\nExperimenting with Distance Metrics\nWe can use the Kmeans method (notice the “K” is capitalized in this function name) from the amap library to specify how we are measuring distance in the K-means algorithm.\n\nset.seed(244)\nk_2_manattan = Kmeans(scale(penguins_reduced), centers = 3, \n                      method = \"manhattan\")\nk_2_euclid = Kmeans(scale(penguins_reduced), centers = 3, \n                    method = \"euclidean\")\nk_2_maxnorm = Kmeans(scale(penguins_reduced), centers = 3, \n                     method = \"maximum\")\n\n\n\nfviz_cluster(k_2_euclid, data = scale(penguins_reduced), \n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_manattan, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_maxnorm, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\n\nTry changing \\(K\\) to equal 3$ in the code chunk above. How do the clusterings using the 3 distance metrics compare? What do you generally observe?\nAnswer. YOUR ANSWER HERE\nModify the code in the chunk above so that we can easily change the value of K (rather than making sure to change K manually in every line). In general coding practices, is called extracting out a constant."
  },
  {
    "objectID": "CrossValidation.html",
    "href": "CrossValidation.html",
    "title": "CrossValidation",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.6     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.4     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(readxl)\nbreastcancer = read_csv(\"breastcancer.csv\")\n\nRows: 569 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): diagnosis\ndbl (31): id, radius_mean, texture_mean, perimeter_mean, area_mean, smoothne...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWrite a brief introduction to cross validation which includes relevant mathematical notation.\nCross-validation is a class of methods that estimate the test error by rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.\n\\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]\nWhat is the goal of cross-validation?\nThe goal of cross-validation is to test the model’s ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give insight into how the model will generalize to an independent data set. Overﬁtting occurs when a model reads too much into chance features and essentially memorizes features of the data used to build it, thus reducing reliability and the ability to generalize to underrepresented populations. Cross-validation helps prevent this by providing a more reliable estimate of how well a model would generalize to unseen data.\nWhat linear models are you considering based on your research question? Pick at least two models to compare:\nE[perimeter_height | concavity_mean] = β0+ β1(concavity_mean)\n\nlm_spec &lt;- linear_reg() %&gt;% set_mode('regression') %&gt;% set_engine('lm')\n\n\ncancer_model&lt;-lm_spec %&gt;% \n  fit(perimeter_mean ~ concavity_mean, data = breastcancer)\n\n\ncancer_model &lt;- ggplot(breastcancer, aes(x = concavity_mean, y = perimeter_mean)) + geom_point()\n\ncancer_model\n\n\n\n\n\n\n\n\n\n# Model _1: 1 predictor (Y = b0 + b1 X)\ncancer_model + geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Model_2: 2 predictors (Y = b0 + b1 X + b2 X^2)\ncancer_model + geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2))\n\n\n\n\n\n\n\n\n\n# Model_3: 10 predictors (Y = b0 + b1 X + b2 X^2 + ... + b10 X^10)\ncancer_model + geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 10))\n\n\n\n\n\n\n\n\nThe models above demonstrate that the more predictors are added, the more the models become overfit to the noise in our data. The models then lose their ability to generalize to new data.\nExplain how you divided your data into its test set and training set.\n\nbreastcancer %&gt;% \n  select(perimeter_mean, texture_mean, smoothness_mean, concavity_mean, diagnosis)\n\n# A tibble: 569 × 5\n   perimeter_mean texture_mean smoothness_mean concavity_mean diagnosis\n            &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    \n 1          123.          10.4          0.118          0.300  M        \n 2          133.          17.8          0.0847         0.0869 M        \n 3          130           21.2          0.110          0.197  M        \n 4           77.6         20.4          0.142          0.241  M        \n 5          135.          14.3          0.100          0.198  M        \n 6           82.6         15.7          0.128          0.158  M        \n 7          120.          20.0          0.0946         0.113  M        \n 8           90.2         20.8          0.119          0.0937 M        \n 9           87.5         21.8          0.127          0.186  M        \n10           84.0         24.0          0.119          0.227  M        \n# ℹ 559 more rows\n\n\nWe have created an 80-20 train-test split below. This ensures that there is a substantial amount of training data in order to evaluate a decently sized testing set.\n\n# Set the random number seed\nset.seed(8)\n\n# Split the data into 80% / 20%\n# Ensure that the sub-samples are similar with respect to mpg\ncancer_split = initial_split(breastcancer, strata = perimeter_mean, prop = .8)\n\n\n# Get the training data from the split\ncancer_train = training(cancer_split)\n\n# Get the testing data from the split\ncancer_test = testing(cancer_split)\n\n\nnrow(breastcancer)\n\n[1] 569\n\nnrow(cancer_train)\n\n[1] 453\n\nnrow(cancer_test)\n\n[1] 116\n\n\n\nlm_spec &lt;- linear_reg() %&gt;% set_mode('regression') %&gt;% set_engine('lm')\n\n\n# Model estimation\ncancer_model &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ concavity_mean, data = breastcancer)\n\n\nmodel_10_train &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ poly(concavity_mean, 10), data = breastcancer)\n\n\n# How well does the TRAINING model predict the TRAINING data?\n# Calculate the training (in-sample) MAE\nmodel_10_train %&gt;% \n  augment(new_data = cancer_train) %&gt;% \n  mae(truth = perimeter_mean, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        12.3\n\n# How well does the TRAINING model predict the TEST data?\n# Calculate the test MAE\nmodel_10_train %&gt;% \n  augment(new_data = cancer_test) %&gt;% \n  mae(truth = perimeter_mean, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        12.7\n\n\nState which error metric you are using (MAE or MSE) and give its formal mathematical definition. Why did you choose this error metric? What are the advantages/disadvantages of using it?\nWe are using MAE, because it provides a fair and interpretable measure of error without disproportionately punishing large outliers, which makes it suitable for medical datasets such as this one, for breast cancer tumors. MAE = (1/n) Σ(i=1 to n) |y_i – ŷ_i|\nImplement k-fold cross validation for k = 10.\n\nset.seed(244)\n\ncancer_model_cv = lm_spec %&gt;%\nfit_resamples(\n  perimeter_mean ~ concavity_mean, \n  resamples = vfold_cv(breastcancer, v = 10), \n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ncancer_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.8      10  0.519  Preprocessor1_Model1\n2 rmse    standard   16.9      10  0.688  Preprocessor1_Model1\n3 rsq     standard    0.530    10  0.0492 Preprocessor1_Model1\n\n\n\ncancer_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\n# A tibble: 10 × 7\n   splits           id     .metric .estimator .estimate .config         .notes  \n   &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;           &lt;list&gt;  \n 1 &lt;split [512/57]&gt; Fold01 mae     standard        14.5 Preprocessor1_… &lt;tibble&gt;\n 2 &lt;split [512/57]&gt; Fold02 mae     standard        14.2 Preprocessor1_… &lt;tibble&gt;\n 3 &lt;split [512/57]&gt; Fold03 mae     standard        13.0 Preprocessor1_… &lt;tibble&gt;\n 4 &lt;split [512/57]&gt; Fold04 mae     standard        11.4 Preprocessor1_… &lt;tibble&gt;\n 5 &lt;split [512/57]&gt; Fold05 mae     standard        14.8 Preprocessor1_… &lt;tibble&gt;\n 6 &lt;split [512/57]&gt; Fold06 mae     standard        10.7 Preprocessor1_… &lt;tibble&gt;\n 7 &lt;split [512/57]&gt; Fold07 mae     standard        11.8 Preprocessor1_… &lt;tibble&gt;\n 8 &lt;split [512/57]&gt; Fold08 mae     standard        13.8 Preprocessor1_… &lt;tibble&gt;\n 9 &lt;split [512/57]&gt; Fold09 mae     standard        13.6 Preprocessor1_… &lt;tibble&gt;\n10 &lt;split [513/56]&gt; Fold10 mae     standard        10.3 Preprocessor1_… &lt;tibble&gt;\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 10 and worst for fold 5. On average, the MAE is 12.758\n\n# 10-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.0      10  0.544  Preprocessor1_Model1\n2 rsq     standard    0.568    10  0.0499 Preprocessor1_Model1\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   11.9      10  0.528  Preprocessor1_Model1\n2 rsq     standard    0.560    10  0.0327 Preprocessor1_Model1\n\n\nDisplay evaluation metrics for your different models in a clean, organized way. This display should include both the estimated CV metric as well as its standard deviation.\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean, data = breastcancer)\n\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean, data = breastcancer)\n\nmodel_3 &lt;- lm_spec %&gt;% \n  fit(perimeter_mean ~ texture_mean + concavity_mean, data=breastcancer)\n\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.557         0.555  16.2      237. 1.96e-99     3 -2391. 4791. 4813.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.565         0.562  16.1      183. 2.36e-100     4 -2386. 4783. 4809.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nmodel_3 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.527         0.525  16.7      315. 1.03e-92     2 -2409. 4827. 4844.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n# IN-SAMPLE MAE for model_1 = \nmodel_1 %&gt;% \n  augment(new_data = breastcancer) %&gt;% \n  mae(truth = perimeter_mean, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        11.9\n\n# IN-SAMPLE MAE for model_2 = \nmodel_2 %&gt;% \n  augment(new_data = breastcancer) %&gt;% \n  mae(truth = perimeter_mean, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        11.9\n\n\nTry different values of k (the tuning parameter). At minimum, try k = n - 1 (LOOCV), and k = 5. Which value of k has the smallest CV error?\n\nmodel_1_loocv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 40), \n    metrics = metric_set(mae)\n  )\n\n\nmodel_1_loocv %&gt;% collect_metrics\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    12.0    40   0.503 Preprocessor1_Model1\n\n\n\n# 5-fold cross-validation for model_1\nset.seed(244)\n\ncancer_model_cv = lm_spec %&gt;%\nfit_resamples(\n  perimeter_mean ~ concavity_mean, \n  resamples = vfold_cv(breastcancer, v = 5), \n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ncancer_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.8       5  0.190  Preprocessor1_Model1\n2 rmse    standard   17.1       5  0.269  Preprocessor1_Model1\n3 rsq     standard    0.527     5  0.0334 Preprocessor1_Model1\n\n\n\ncancer_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\n# A tibble: 5 × 7\n  splits            id    .metric .estimator .estimate .config          .notes  \n  &lt;list&gt;            &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;list&gt;  \n1 &lt;split [455/114]&gt; Fold1 mae     standard        12.7 Preprocessor1_M… &lt;tibble&gt;\n2 &lt;split [455/114]&gt; Fold2 mae     standard        12.9 Preprocessor1_M… &lt;tibble&gt;\n3 &lt;split [455/114]&gt; Fold3 mae     standard        13.5 Preprocessor1_M… &lt;tibble&gt;\n4 &lt;split [455/114]&gt; Fold4 mae     standard        12.5 Preprocessor1_M… &lt;tibble&gt;\n5 &lt;split [456/113]&gt; Fold5 mae     standard        12.6 Preprocessor1_M… &lt;tibble&gt;\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 4 and worst for fold 3.\n\n# 5-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 5), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n# 5-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 5), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n# 5-fold cross-validation for model_3\nset.seed(253)\nmodel_3_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean,\n    resamples = vfold_cv(breastcancer, v = 5), \n    metrics = metric_set(mae, rsq)\n  )\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.0       5  0.236  Preprocessor1_Model1\n2 rsq     standard    0.569     5  0.0396 Preprocessor1_Model1\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   11.9       5  0.599  Preprocessor1_Model1\n2 rsq     standard    0.556     5  0.0418 Preprocessor1_Model1\n\nmodel_3_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.8       5  0.574  Preprocessor1_Model1\n2 rsq     standard    0.521     5  0.0403 Preprocessor1_Model1\n\n\n\n# 8-fold cross-validation\nset.seed(244)\n\ncancer_model_cv = lm_spec %&gt;%\nfit_resamples(\n  perimeter_mean ~ concavity_mean, \n  resamples = vfold_cv(breastcancer, v = 8), \n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ncancer_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.8       8  0.399  Preprocessor1_Model1\n2 rmse    standard   16.9       8  0.765  Preprocessor1_Model1\n3 rsq     standard    0.517     8  0.0406 Preprocessor1_Model1\n\n\n\ncancer_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\n# A tibble: 8 × 7\n  splits           id    .metric .estimator .estimate .config           .notes  \n  &lt;list&gt;           &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n1 &lt;split [497/72]&gt; Fold1 mae     standard        14.8 Preprocessor1_Mo… &lt;tibble&gt;\n2 &lt;split [498/71]&gt; Fold2 mae     standard        11.2 Preprocessor1_Mo… &lt;tibble&gt;\n3 &lt;split [498/71]&gt; Fold3 mae     standard        13.0 Preprocessor1_Mo… &lt;tibble&gt;\n4 &lt;split [498/71]&gt; Fold4 mae     standard        12.2 Preprocessor1_Mo… &lt;tibble&gt;\n5 &lt;split [498/71]&gt; Fold5 mae     standard        13.5 Preprocessor1_Mo… &lt;tibble&gt;\n6 &lt;split [498/71]&gt; Fold6 mae     standard        11.8 Preprocessor1_Mo… &lt;tibble&gt;\n7 &lt;split [498/71]&gt; Fold7 mae     standard        12.5 Preprocessor1_Mo… &lt;tibble&gt;\n8 &lt;split [498/71]&gt; Fold8 mae     standard        13.2 Preprocessor1_Mo… &lt;tibble&gt;\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 2 and worst for fold 1.\n\n# 8-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean + concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 8), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n# 8-fold cross-validation for model_2\nset.seed(244)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    perimeter_mean ~ texture_mean * concavity_mean + smoothness_mean,\n    resamples = vfold_cv(breastcancer, v = 8), \n    metrics = metric_set(mae, rsq)\n  )\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   11.9       8  0.461  Preprocessor1_Model1\n2 rsq     standard    0.557     8  0.0418 Preprocessor1_Model1\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   12.0       8  0.457  Preprocessor1_Model1\n2 rsq     standard    0.553     8  0.0403 Preprocessor1_Model1\n\n\nThe CV errors seem to be fairly consistent between 10-fold, 5-fold, 8-fold and LOOCV. Although a slim difference, the 10-fold cross-validation measure provides the smallest CV error.\nSelect your final model based on which one has the smallest CV error.\nFor model_1, it looks like the MAE is roughly similar for when it’s measured in-sample (11.82) versus when it’s tested on “new” data (each test fold held out) (11.887 for 10-fold, 11.92 for 5-fold and LOOCV, and 11.875 for 8-fold CV). Model_2 also has roughly similar MAE for in-sample (11.801) versus CV data (11.93 for 10-fold, 12.01 for 5-fold, and 11.901 for 8-fold CV). Althought the difference is small, it seems better to pick the first model."
  }
]